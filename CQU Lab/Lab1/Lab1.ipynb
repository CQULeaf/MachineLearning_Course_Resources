{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习第一次实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务一：理解对数几率回归（Logistic Regression）算法原理\n",
    "\n",
    "### 对数几率回归的核心\n",
    "\n",
    "- **概率模型**：对数几率回归的目标是预测给定输入数据点属于某一类的概率。它通过将数据特征的线性组合输入到逻辑函数（通常是Sigmoid函数）中，后将输出转换为概率值。\n",
    "\n",
    "- **Sigmoid函数**：\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  其中，$z$是特征与权重的线性组合。Sigmoid函数的输出范围是$(0,1)$，非常适合表示概率。对于给定的输入特征$x$，$z$可以表示为$z = w^T x + b$，其中$w$是权重向量，$b$是偏置项。\n",
    "\n",
    "- **决策边界**：对数几率回归模型通过预测概率来做出分类决策。通常情况下，如果预测概率大于或等于$0.5$，模型会将数据点分类为正类（标签为$1$）；否则，分类为负类（标签为$0$）。\n",
    "\n",
    "### 损失函数与参数优化\n",
    "\n",
    "- **损失函数（对数损失）**：在对数几率回归中，使用对数损失函数来度量模型预测值与实际标签之间的差异。对于单个数据点，损失函数定义为：\n",
    "  $$\n",
    "  L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "  $$\n",
    "  其中，$y$是真实标签，$\\hat{y}$是预测概率。对于整个数据集，损失是所有单个损失的平均值。\n",
    "\n",
    "- **参数优化（梯度下降）**：对数几率回归使用梯度下降算法来优化模型参数（权重$w$和偏置$b$），以最小化损失函数。梯度下降是一种迭代优化算法，通过不断更新参数来逐步减少损失函数的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务二：编程实现对数几率回归算法\n",
    "\n",
    "以下是对数几率回归算法设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # 梯度下降\n",
    "        for _ in range(self.num_iterations):\n",
    "            # 计算模型预测值\n",
    "            model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self._sigmoid(model)\n",
    "            \n",
    "            # 计算梯度\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1 / num_samples) * np.sum(predictions - y)\n",
    "            \n",
    "            # 更新参数\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self._sigmoid(model)\n",
    "        prediction_class = [1 if i > 0.5 else 0 for i in predictions]\n",
    "        return prediction_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务三：将算法应用于具体数据集来完成分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先研究鸢尾花数据集分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

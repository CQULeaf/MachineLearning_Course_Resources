# 机器学习第十章作业

10.1 编程实现 k 近邻分类器，在西瓜数据集 3.0α 上比较其分类边界与决策树分类边界之异同

```python
import numpy as np
import pandas as pd
import bisect
import matplotlib.pyplot as plt
from sklearn import tree


class Node:
    def __init__(self):
        self.father = None
        self.left = None
        self.right = None
        self.split_point = None
        self.feature = None  # Index of the feature used for split at this node

    def __str__(self):
        return f"feature: {self.feature}, split point: {self.split_point}"

    @property
    def is_leaf(self):
        return self.left is None and self.right is None

    @property
    def brother(self):
        if self.father is None:
            return None
        return self.father.left if self.father.right is self else self.father.right


class KNN:
    def __init__(self, k=5):
        self.k = k
        self.m = None
        self.n = None
        self.root = None

    def fit(self, X, y):
        self.m, self.n = X.shape
        self.root = self._generate_kd_tree(X, y)
        return self

    def predict(self, X):
        results = np.empty(X.shape[0], dtype=int)
        for i in range(X.shape[0]):
            _, knn_node = self._knn_search(X[i])
            results[i] = np.argmax(np.bincount([node.split_point[1] for node in knn_node]))
        return results

    def _generate_kd_tree(self, X, y, feature=0, father=None):
        if len(X) == 0:
            return None

        node = Node()
        node.father = father
        node.feature = feature

        if len(X) == 1:
            node.split_point = (X[0], y[0])
            return node

        median_index = np.argsort(X[:, feature])[len(X) // 2]
        node.split_point = (X[median_index], y[median_index])

        left_indices = np.arange(len(X)) < median_index
        right_indices = np.arange(len(X)) > median_index

        node.left = self._generate_kd_tree(X[left_indices], y[left_indices], (feature + 1) % self.n, node)
        node.right = self._generate_kd_tree(X[right_indices], y[right_indices], (feature + 1) % self.n, node)

        return node

    def _knn_search(self, x, node=None, knn_dist=None, knn_node=None):
        if node is None:
            node = self.root
        if knn_dist is None or knn_node is None:
            knn_dist, knn_node = [], []

        current_node = self._get_leaf(x, node)
        while current_node is not None:
            distance = np.linalg.norm(x - current_node.split_point[0])
            if len(knn_dist) < self.k or distance < knn_dist[-1]:
                bisect.insort(knn_dist, distance)
                bisect.insort(knn_node, current_node)
                if len(knn_dist) > self.k:
                    knn_dist.pop()
                    knn_node.pop()
            if current_node.brother and (len(knn_dist) < self.k or distance < np.abs(x[current_node.feature] - current_node.split_point[0][current_node.feature])):
                self._knn_search(x, current_node.brother, knn_dist, knn_node)
            current_node = current_node.father
        return knn_dist, knn_node

    def _get_leaf(self, x, node):
        while not node.is_leaf:
            node = node.left if x[node.feature] < node.split_point[0][node.feature] else node.right
        return node


def plot_decision_boundary(X, y, classifier, title):
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1

    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

    plt.contour(xx, yy, Z, levels=[0.5], colors='orange', linewidths=1)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='c', label='Positive')
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='lightcoral', label='Negative')

    plt.title(title)
    plt.legend()
    plt.show()


if __name__ == '__main__':
    data_path = '../data/watermelon3_0a_Ch.txt'
    data = pd.read_table(data_path, delimiter=' ')
    X = data.iloc[:, :2].values
    y = data.iloc[:, 2].values

    knn = KNN(3)
    knn.fit(X, y)
    plot_decision_boundary(X, y, knn, 'KNN Decision Boundary')

    decision_tree = tree.DecisionTreeClassifier().fit(X, y)
    plot_decision_boundary(X, y, decision_tree, 'Decision Tree Boundary')
```

10.3 在对高维数据降维之前应先进行“中心化”, 常见的是将协方差矩阵 $ XX^T $ 转化为 $ XHH^TX^T $ ，其中 $ H = I - \frac{1}{m} 11^T $ ，试析其效果

答：
$ XHH^TX^T $ 就是直接求未中心化的数据样本的协方差矩阵。

表示值全为 $1$ 的 $m$ 维列向量 , $ 11^T $ 则表示值全为 $1$ 的 $ m \times m $ 维矩阵，令 $ S = \frac{1}{m} X11^T $ 为 $ d \times m $ 维矩阵，其中对于任意 $ k = 1, 2, ...,m $，都有： $ S_{ik} = \frac{1}{m} \sum_{j=1}^{m} {X_{ij}} $, 即 S 中第 i 行元素都相同，值等于 X 特征 i 的均值 $ u_i $,所以 $ XH = X - \frac{1}{m} X11^T = X - S $ 即为 “中心化” 后的数据样本。

另外，这里题中表述感觉有点奇异，实际上，$ XX^T $ 是协方差矩阵的前提是 X 经过 “中心化”，当然严格来说其实还要再除以 m ，参考协方差矩阵,对于$ X = (X_1, X_2, ..., X_m) \in R^{d \times m}$, 其协方差矩阵 $ \varSigma $ 有： $ \varSigma_{ij} = cov(X_i, X_j) = E[(X_i - u_i)(X_j - u_j)] = \frac{1}{m} \sum_{k=1}^{m} {(X_{ik} - u_i)(X_{jk} - u_j)}$， 若 X 中心化后，则 $ u_i = 0 $， 于是 $ \varSigma_{ij} = \frac{1}{m} \sum_{k=1}^{m} X_{ik} X_{jk} =\frac{1}{m} X_iX_j^T \Longrightarrow \varSigma = \frac{1}{m} XX^T$ , 这也是为什么要进行中心化的原因。

10.5 降维中涉及的投影矩阵通常要求是正交的.试述正交、非正交投影矩阵用于降维的优缺点

**正交投影矩阵：**

优点：

1. **保留距离和角度**：正交投影保持了向量间的夹角和长度。这是因为正交变换是等距映射，即内积保持不变，从而保持了数据点间的相对距离和角度，减少了数据投影后的失真。
2. **避免数据重叠和混叠**：由于保持了距离和角度，正交降维有助于在降维后的空间中避免不同数据点的重叠和混叠，使得数据的可分性更好。
3. **数据解释性**：正交投影通常使得投影后的数据具有更好的解释性。例如，在PCA中，各主成分是正交的，代表了数据在正交方向上的独立变异，方便解释各方向的数据变化。

缺点：

1. **可能忽略重要特征**：在某些情况下，如果重要特征不在主成分的方向上，正交约束可能导致这些特征被忽略。
2. **计算复杂度**：计算正交投影矩阵（如PCA中的奇异值分解）可能比较耗时，尤其是在面对大规模数据集时。

**非正交投影矩阵：**

优点：

1. **灵活性**：非正交投影矩阵提供了更大的灵活性，在处理那些正交方法难以捕捉的复杂内在结构时可能更有效。
2. **适用性**：对于非线性降维，如核PCA、局部线性嵌入（LLE）等，非正交投影更能捕捉数据的本质特征。

缺点：

1. **可能引入噪声**：非正交投影可能会增加投影数据的噪声，因为它可能会增强数据中的不相关变异。
2. **解释性差**：非正交投影通常难以直接解释，因为各投影方向之间可能存在依赖关系，不像正交投影那样清晰明了。
3. **计算成本**：虽然正交投影也有计算负担，但非正交投影在优化和调整时可能需要更复杂的算法和更多的计算资源。

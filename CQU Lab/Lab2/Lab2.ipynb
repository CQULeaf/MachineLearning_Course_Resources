{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习第二次实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务一：理解并描述决策树分类、回归算法原理\n",
    "\n",
    "决策树是一种**监督**学习算法，广泛应用于分类和回归任务。其核心思想是基于数据属性进行**递归**划分，构建一个树状模型。在决策树中，内部节点代表一个属性上的测试，每个分支代表测试的一个结果，叶节点代表最终的决策结果。\n",
    "\n",
    "### 决策树分类算法原理:\n",
    "\n",
    "1. **特征选择**：常用的特征选择标准包括信息增益（ID3），信息增益率（C4.5）和基尼指数（CART）。信息增益高的特征具有更强的分类能力，基尼指数则用于CART算法，衡量数据的不纯度，基尼指数越小，数据纯度越高。\n",
    "\n",
    "    - **信息增益**：计算每个特征分割前后的信息熵变化，选择使信息熵降低最多的特征。\n",
    "    - **信息增益率**：对信息增益结果进行归一化处理，解决信息增益偏向于选择取值较多的特征的问题。\n",
    "    - **基尼指数**：CART算法使用，反映了从数据集中随机抽取两个样本，其类别标签不一致的概率。\n",
    "\n",
    "2. **树的构建**：从根节点开始，使用特征选择方法选择最佳特征，根据该特征的不同取值构建分支。对每个分支递归执行同样的过程，直至满足停止条件，如节点中样本数小于最小分割样本数、节点纯度达到阈值或达到预设的最大深度。\n",
    "\n",
    "3. **剪枝**：构建完成后，通过剪枝来避免过拟合。剪枝分为预剪枝和后剪枝，预剪枝是在构建树的过程中提前停止树的增长，后剪枝则是先构建完整的树，然后删除掉一些子树或节点。\n",
    "\n",
    "### 决策树回归算法原理：\n",
    "\n",
    "1. **特征选择**：通常基于最小化均方误差（MSE）或总方差减少来选择最佳分割特征和分割点。\n",
    "\n",
    "2. **树的构建**：选择最佳分割特征后，按此特征的值分割数据集，生成两个子节点，并对每个子节点递归重复此过程，直到满足停止条件。\n",
    "\n",
    "3. **预测**：对于回归树的叶节点，其预测值通常是到达该叶节点的所有样本目标值的均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

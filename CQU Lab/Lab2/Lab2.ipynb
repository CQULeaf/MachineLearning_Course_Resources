{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习第二次实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 理解并描述决策树分类、回归算法原理\n",
    "\n",
    "决策树是一种**监督**学习算法，广泛应用于分类和回归任务。其核心思想是基于数据属性进行**递归**划分，构建一个树状模型。在决策树中，内部节点代表一个属性上的测试，每个分支代表测试的一个结果，叶节点代表最终的决策结果。\n",
    "\n",
    "### 决策树分类算法原理:\n",
    "\n",
    "1. **特征选择**：常用的特征选择标准包括信息增益（ID3），信息增益率（C4.5）和基尼指数（CART）。信息增益高的特征具有更强的分类能力，基尼指数则用于CART算法，衡量数据的不纯度，基尼指数越小，数据纯度越高。\n",
    "\n",
    "    - **信息增益**：计算每个特征分割前后的信息熵变化，选择使信息熵降低最多的特征。\n",
    "    - **信息增益率**：对信息增益结果进行归一化处理，解决信息增益偏向于选择取值较多的特征的问题。\n",
    "    - **基尼指数**：CART算法使用，反映了从数据集中随机抽取两个样本，其类别标签不一致的概率。\n",
    "\n",
    "2. **树的构建**：从根节点开始，使用特征选择方法选择最佳特征，根据该特征的不同取值构建分支。对每个分支递归执行同样的过程，直至满足停止条件，如节点中样本数小于最小分割样本数、节点纯度达到阈值或达到预设的最大深度。\n",
    "\n",
    "3. **剪枝**：构建完成后，通过剪枝来避免过拟合。剪枝分为预剪枝和后剪枝，预剪枝是在构建树的过程中提前停止树的增长，后剪枝则是先构建完整的树，然后删除掉一些子树或节点。\n",
    "\n",
    "### 决策树回归算法原理：\n",
    "\n",
    "1. **特征选择**：通常基于最小化均方误差（MSE）或总方差减少来选择最佳分割特征和分割点。\n",
    "\n",
    "2. **树的构建**：选择最佳分割特征后，按此特征的值分割数据集，生成两个子节点，并对每个子节点递归重复此过程，直到满足停止条件。\n",
    "\n",
    "3. **预测**：对于回归树的叶节点，其预测值通常是到达该叶节点的所有样本目标值的均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树分类与回归算法设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树分类器\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    left_indices = X[:, feature] < threshold\n",
    "    right_indices = ~left_indices\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_feature, best_threshold, best_gain = None, None, float(\"-inf\")\n",
    "    base_entropy = calculate_entropy(y)\n",
    "    for feature in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            _, _, y_left, y_right = split_dataset(X, y, feature, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            entropy_left, entropy_right = calculate_entropy(y_left), calculate_entropy(y_right)\n",
    "            weighted_entropy = (len(y_left) / len(y)) * entropy_left + (len(y_right) / len(y)) * entropy_right\n",
    "            information_gain = base_entropy - weighted_entropy\n",
    "            if information_gain > best_gain:\n",
    "                best_feature, best_threshold, best_gain = feature, threshold, information_gain\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=None):\n",
    "    if len(np.unique(y)) == 1 or (max_depth is not None and depth == max_depth):\n",
    "        value = np.argmax(np.bincount(y))\n",
    "        return DecisionTreeNode(value=value)\n",
    "    feature, threshold = best_split(X, y)\n",
    "    if feature is None:\n",
    "        return DecisionTreeNode(value=np.argmax(np.bincount(y)))\n",
    "    X_left, X_right, y_left, y_right = split_dataset(X, y, feature, threshold)\n",
    "    left = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    right = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    return DecisionTreeNode(feature=feature, threshold=threshold, left=left, right=right)\n",
    "\n",
    "def predict(node, X):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if X[node.feature] < node.threshold:\n",
    "        return predict(node.left, X)\n",
    "    else:\n",
    "        return predict(node.right, X)\n",
    "\n",
    "def decision_tree_predict(tree, X):\n",
    "    return np.array([predict(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树回归器\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def split_dataset(X, y, feature, threshold):\n",
    "    left_indices = X[:, feature] < threshold\n",
    "    right_indices = ~left_indices\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_feature, best_threshold, best_mse = None, None, float(\"inf\")\n",
    "    for feature in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            X_left, X_right, y_left, y_right = split_dataset(X, y, feature, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            mse_left = calculate_mse(y_left, np.mean(y_left))\n",
    "            mse_right = calculate_mse(y_right, np.mean(y_right))\n",
    "            weighted_mse = (len(y_left) / len(y)) * mse_left + (len(y_right) / len(y)) * mse_right\n",
    "            if weighted_mse < best_mse:\n",
    "                best_feature, best_threshold, best_mse = feature, threshold, weighted_mse\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=None):\n",
    "    if len(np.unique(y)) == 1 or (max_depth is not None and depth == max_depth):\n",
    "        value = np.mean(y)\n",
    "        return DecisionTreeNode(value=value)\n",
    "    feature, threshold = best_split(X, y)\n",
    "    if feature is None:\n",
    "        return DecisionTreeNode(value=np.mean(y))\n",
    "    X_left, X_right, y_left, y_right = split_dataset(X, y, feature, threshold)\n",
    "    left = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    right = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "    return DecisionTreeNode(feature=feature, threshold=threshold, left=left, right=right)\n",
    "\n",
    "def predict(node, X):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if X[node.feature] < node.threshold:\n",
    "        return predict(node.left, X)\n",
    "    else:\n",
    "        return predict(node.right, X)\n",
    "\n",
    "def decision_tree_predict(tree, X):\n",
    "    return np.array([predict(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集获取\n",
    "\n",
    "### 用于分类的数据集\n",
    "\n",
    "1. 鸢尾花数据集（Iris）：包含150个样本，分为3个类别，每个类别50个样本。每个样本有4个特征，分别是花瓣和花萼的长度和宽度。\n",
    "2. 葡萄酒数据集（Wine）：包含178个样本，分为3个类别，代表了三种不同的意大利葡萄酒。有13个特征，这些特征是从葡萄酒的化学成分分析中得出的，比如酒精度、苹果酸含量等。\n",
    "\n",
    "### 用于回归的数据集\n",
    "\n",
    "1. 波士顿房价数据集（Boston Housing）：包含506个样本和13个特征，目标是预测波士顿地区的房屋价格中位数。\n",
    "2. 糖尿病数据集（Diabetes）：包含442个样本和10个基于生理特征的特征（年龄、性别、BMI等），目标是一年后病情的量化测量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集获取\n",
    "\n",
    "from sklearn.datasets import load_iris, load_wine, load_diabetes\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "wine = load_wine()\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.to_csv('iris.csv', index=False)\n",
    "\n",
    "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_df['target'] = wine.target\n",
    "wine_df.to_csv('wine.csv', index=False)\n",
    "\n",
    "diabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "diabetes_df['target'] = diabetes.target\n",
    "diabetes_df.to_csv('diabetes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践：将决策树算法用于具体数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "# 一、使用决策树分类器对鸢尾花数据集进行分类\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris_df = pd.read_csv('data/iris.csv')\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X, y = iris_df.iloc[:, :-1].values, iris_df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 下面想使用随机森林算法对鸢尾花数据集进行分类\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'n_estimators': 30}\n",
      "0.9583333333333334\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索调整随机森林的参数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30, 40, 50],\n",
    "    'max_depth': [3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 鸢尾花数据集结论\n",
    "\n",
    "1. 首先调用决策树分类器对鸢尾花数据集进行分类，分类准确率为 $ 100\\% $\n",
    "2. 然后使用随机森林算法再次进行分类，分类准确率仍为 $ 100\\% $\n",
    "3. 最后使用网格搜索调整随机森林的参数，得出最优参数为：最大深度 $3$，决策树个数 $30$；最优得分为 $0.958$。在此参数下，分类准确率仍为 $ 100\\% $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
      "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "\n",
      "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   od280/od315_of_diluted_wines  proline  target  \n",
      "0                          3.92   1065.0       0  \n",
      "1                          3.40   1050.0       0  \n",
      "2                          3.17   1185.0       0  \n",
      "3                          3.45   1480.0       0  \n",
      "4                          2.93    735.0       0  \n"
     ]
    }
   ],
   "source": [
    "# 二、使用决策树回归器对葡萄酒数据集进行分类\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "wine_df = pd.read_csv('data/wine.csv')\n",
    "print(wine_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "X, y = wine_df.iloc[:, :-1].values, wine_df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# 下面想使用XGBoost对葡萄酒数据集进行分类\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'n_estimators': 30}\n",
      "0.9573891625615765\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索算法进行参数优化\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50, 100],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 葡萄酒数据集结论\n",
    "\n",
    "1. 首先调用决策树分类器对葡萄酒数据集进行分类，分类准确率为 $ 94.4\\% $\n",
    "2. 然后使用XGBoost算法再次进行分类，分类准确率提升至 $ 97.2\\% $\n",
    "3. 最后使用网格搜索调整XGB的参数，得出最优参数为：最大深度 $3$，决策树个数 $30$；最优得分为 $0.957$。在此参数下，分类准确率为 $ 97.2\\% $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "        B  LSTAT  MEDV  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "4  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "# 三、使用决策树回归器对波士顿房价数据集进行回归\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "house_df = pd.read_csv('data/house.csv')\n",
    "print(house_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.08343137254902\n"
     ]
    }
   ],
   "source": [
    "X, y = house_df.iloc[:, :-1].values, house_df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.560527271813469\n"
     ]
    }
   ],
   "source": [
    "# 下面想使用XGBoost对波士顿房价数据集进行回归\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 50}\n",
      "0.8431263858254091\n",
      "6.588402908612317\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索算法进行参数优化\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50, 100],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 波士顿房价数据集结论\n",
    "\n",
    "1. 首先调用决策树回归器对波士顿房价数据集进行回归分析，MSE为 $ 11.08 $\n",
    "2. 然后使用XGBoost算法再次进行回归分析，MSE优化至 $ 6.56 $\n",
    "3. 最后使用网格搜索调整XGB的参数，得出最优参数为：最大深度 $6$，决策树个数 $50$；最优得分为 $0.843$。在此参数下，MSE为 $ 6.59 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019907 -0.017646   151.0  \n",
      "1 -0.039493 -0.068332 -0.092204    75.0  \n",
      "2 -0.002592  0.002861 -0.025930   141.0  \n",
      "3  0.034309  0.022688 -0.009362   206.0  \n",
      "4 -0.002592 -0.031988 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "# 四、使用决策树回归器对糖尿病数据集进行回归\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "diabetes_df = pd.read_csv('data/diabetes.csv')\n",
    "print(diabetes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4902.08988764045\n"
     ]
    }
   ],
   "source": [
    "X, y = diabetes_df.iloc[:, :-1].values, diabetes_df.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3058.205806741573\n"
     ]
    }
   ],
   "source": [
    "# 下面想使用随机森林对糖尿病数据集进行回归\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor()\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 50}\n",
      "0.42437125391992153\n",
      "2869.5343412155153\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索算法进行参数优化\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50, 100],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_samples_split': [2, 4, 6, 8],\n",
    "    'min_samples_leaf': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 糖尿病数据集结论\n",
    "\n",
    "1. 首先调用决策树回归器对糖尿病数据集进行回归分析，MSE为 $ 4902.09 $\n",
    "2. 然后使用随机森林算法再次进行回归分析，MSE优化至 $ 3058.21 $\n",
    "3. 最后使用网格搜索调整随机森林的参数，得出最优参数为：最大深度 $3$，分裂内部节点所需的最小样本数为 $4$，叶节点所需的最小样本数 $4$，决策树个数 $50$；最优得分为 $0.424$。在此参数下，MSE为 $ 2869.53 $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
